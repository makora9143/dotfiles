function isShowHeader() {
return 1;
}

function getFileName() {
return "/Users/makora/Documents/phd_documents/syllabus/comment.md";
}

function getFileType() {
return "markdown";
}

function getLastModified() {
return "2016/05/11 (水) 12:11:42";
}

function getContent() {
return "# Original Comment\n\n## 一つ目\n\n### [評価項目]\n\n総合評価：3.0/5.0\n\n- 授業科目としての妥当性(20％)　　：15/20\n- 既存知識の体系づけの妥当性(30％)：10/30\n- 科目の構成の構想力(30％)　　　　：20/30\n- 内容のわかりやすさ、説得性(20％)：15/20\n\n\n### [コメント]\n\n- 深層学習だけに特化せずに、機械学習全般について取り扱ってほしい。\n- 表面的な紹介ではなく、背景にある概念が判るように体系付けてください。\n- 演習がなかったら、数回で終わるような内容に思います。\n- シラバスは、あなたが研究者としてその分野の知識を正しく持っているかを把握するためのものなので、一種のサーベイ論文を作成してください。深層学習だけでなく、機械学習全般について取り上げ、それぞれ研究論文を参照するなど、濃い内容のものにしてください。実際に授業で教えることは想定することはないので、演習やコンテストなどは必要ありません。\n\n## 二つ目\n\n\n### [評価項目]\n\n総合評価：3.2/5.0\n\n- 授業科目としての妥当性(20％)　　：18/20\n- 既存知識の体系づけの妥当性(30％)：15/30\n- 科目の構成の構想力(30％)　　　　：15/30\n- 内容のわかりやすさ、説得性(20％)：16/20\n\n### [コメント]\n\n- 大学院レベルなので、線形代数やプログラミングの復習は不要ではないか。それより機械学習や人工知能一般の発展経過や理論的基礎などを概観する方がよいと思われる。\n- 週ごとの授業内容をもっと詳しく書いてください。\n\n# Merge Comment\n\n## [評価項目]\n\n総合評価：6.2/10.0\n\n- 授業科目としての妥当性(20％)　　：33/40\n- 既存知識の体系づけの妥当性(30％)：25/60\n- 科目の構成の構想力(30％)　　　　：35/60\n- 内容のわかりやすさ、説得性(20％)：31/40\n\n## コメント\n\n- 深層学習だけでなく，機械学習全般や人工知能一般の発展経過，理論的基礎などを扱った方が良いのでは？\n- 演習はいらない．\n\n## それに対する考え\n\n- 深層学習だけでもかなり奥が深い．\n - 画像分野・自然言語・確率モデルなど様々な領域でたくさんある\n- 人工知能とは何を指すのか？（歴史を語れるほどの知識は，，，）\n- 理論的基礎と線形代数（確率論など？）の違いは？\n - マルコフモデルや確率分布，多様体は違うもの？\n - 前提知識を揃えるのが難しい\n- 機械学習全般の研究論文とは？\n - SVMやナイーブベイズの原著？\n - それとも応用研究？→無限にある\n \n- 深層学習について何も知らない人の気がする\n - 最適化だけでもいろんなアルゴリズムがある\n - 理論を話せば当然中身は薄くなる（理解するのに時間がかかる）\n\n上田先生の本でも以下の通り．\n\n- 第1章 ベイズ統計学\n- 第2章 事前確率と事後確率\n- 第3章 ベイズ決定則\n- 第4章 パラメータ推定\n- 第5章 教師付き学習と教師なし学習\n- 第6章 EMアルゴリズム\n- 第7章 マルコフモデル\n- 第8章 隠れマルコフモデル\n- 第9章 混合分布のパラメータ推定\n- 第10章 クラスタリング\n- 第11章 ノンパラメトリックベイズモデル\n- 第12章 ディリクレ過程混合モデルによるクラスタリング\n- 第13章 共クラスタリング\n\n各授業で1章やっても終わらないくらい難しいが，おそらく同じ評価者なら6時間くらいで終わるとか言われそう\n\n- 既存知識の体系づけ，構成の構想力が弱い\n- シラバスとサーベイ論文の違い（バランスがわからない）\n\n## これらを踏まえて\n\n- もう一度そのまま出す\n - 別の人が見れば評価が異なる？\n - タイトルとガイダンスだけ変更？\n- シラバスっぽさをなくしていく\n - 課題やコンテストをなくしていく\n - 理論的基礎？を入れる？\n- 根本から作り直す\n - タイトルは「人工知能論」「機械学習概論」とか？\n - 識別モデルと生成モデルという観点？\n - 最小二乗誤差からSVMや決定木，ロジスティック回帰をやる\n - 教師あり学習と教師なし学習，強化学習という観点で分ける\n - 全15回だから4，4，3，4でやる？（あり，なし，強化，深層）\n\n# 各週の内容について\n\n1. ガイダンス：人工知能の歴史\n 1. 探索・推論→知識ベース→機械学習・表現学習\n 1. 松尾先生のスライド参考→参考文献は？\n1. 機械学習：教師あり学習\n 1. 問題意識\n1. 深層学習：CNN\n1. 深層学習：RNN\n1. 深層学習：Pre-training\n1. 深層学習：生成モデル\n1. 深層学習：応用\n1. 深層学習\n1. 深層学習\n1. 深層学習\n1. 人工知能の未来\n\n\n1. 機械学習：教師なし学習\n1. 機械学習：強化学習\n\n\n# 構成\n1. ガイダンス：人工知能の歴史\n1. 機械学習：枠組み→機械学習を適用するための問題設定\n 1. 全体像\n1. 機械学習\n 1. 最適化（正規方程式）\n 1. 正則化\n1. 機械学習：パラメトリック・ノンパラメトリック\n 1. モデルについて（教師あり）\n1. 機械学習：パラメトリック・ノンパラメトリック\n 1. モデルについて（教師なし）\n1. MLP-ニューラルネットワーク\n 1. MLPの正則化\n 1. MLPの最適化(SGD, Adamなど)\n1. 深層学習：Pre-training(Autoencoder, RBM)\n1. 表現学習：PCA→AE\n1. 深層学習：CNN\n1. 深層学習：画像認識へのCNN\n 1. 画像認識の問題設定\n1. 深層学習：生成モデル(VAE, GAN)\n1. 深層学習：RNN\n1. 深層学習：自然言語処理のRNN\n 1. 自然言語処理の問題設定\n1. 深層学習：実世界への応用\n 1. DQN\n 1. PFI\n1. 人工知能の未来\n\n## 参考文献\n\nPRML・上田先生・the machine learning\n原著論文";
}
